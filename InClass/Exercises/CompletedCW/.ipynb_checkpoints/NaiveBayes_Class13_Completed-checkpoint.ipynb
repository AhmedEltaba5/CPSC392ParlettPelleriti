{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.7g'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB # Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "from sklearn.model_selection import cross_val_score # cross validation metrics\n",
    "from sklearn.model_selection import cross_val_predict # cross validation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#set precision to get rid of some scientific notation\n",
    "%precision %.7g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## 0. Together\n",
    "\n",
    "### 0.0 Probability and Conditional Probability\n",
    "\n",
    "#### *Question*\n",
    "What is the difference between a conditional probability and a regular probability (for example $P(dog)$ vs $P(dog | kids)$)?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n",
    "Using the table below, how would we calculate $P(dog)$? $P(dog | kids)$?\n",
    "\n",
    "|           | dog | kid |\n",
    "|-----------|-----|-----|\n",
    "| Person 1  | 1   | 1   |\n",
    "| Person 2  | 1   | 1   |\n",
    "| Person 3  | 1   | 0   |\n",
    "| Person 4  | 1   | 1   |\n",
    "| Person 5  | 1   | 0   |\n",
    "| Person 6  | 1   | 1   |\n",
    "| Person 7  | 0   | 0   |\n",
    "| Person 8  | 0   | 1   |\n",
    "| Person 9  | 0   | 1   |\n",
    "| Person 10 | 0   | 1   |\n",
    "\n",
    "Using the table below, how would we calculate $P(dog | kids, over20)$?\n",
    "\n",
    "|           | dog | kid | over20 |\n",
    "|-----------|-----|-----|--------|\n",
    "| Person 1  | 1   | 1   | 1      |\n",
    "| Person 2  | 1   | 1   | 0      |\n",
    "| Person 3  | 1   | 0   | 0      |\n",
    "| Person 4  | 1   | 1   | 1      |\n",
    "| Person 5  | 1   | 0   | 1      |\n",
    "| Person 6  | 1   | 1   | 1      |\n",
    "| Person 7  | 0   | 0   | 1      |\n",
    "| Person 8  | 0   | 1   | 0      |\n",
    "| Person 9  | 0   | 1   | 0      |\n",
    "| Person 10 | 0   | 1   | 1      |\n",
    "\n",
    "### 0.1 Naive\n",
    "Naive Bayes is a classification algorithm which assumes (incorrectly) that within a group/class, the probability of a combination of predictor values (like $P(diabetic, obese, smoker)$) is equal to the product of the individual predictor probabilities. In other words, it assumes that they are *independent* and that knowing someone is a smoker does *not* affect the probability of being diabetic. In mathematical terms, for example:\n",
    "\n",
    "$$P(D,O,S) = P(D) * P(O) * P(S)$$\n",
    "\n",
    "In real life we know that this independence is very unlikely (hence: *naive*). But it turns out that this inapproporiate assumption doesn't usually have a huge effect on the accuracy of the model, and it saves a LOT of computational time because we can simply calculate independent probabilities and multiply them, rather than calculating complex conditional probabilities.\n",
    "\n",
    "### 0.2 Bayes\n",
    "The Bayes part of the Naive Bayes algorithm refers to the fact that we calculate \"scores\" that measure how likely a data point is to belong to some class, $C$. These \"scores\" are proportional to the probability of a data point belonging to class $C$. Once we have a \"score\" for each possible category, we choose whichever category has the highest score. \n",
    "\n",
    "The \"score\" is based on Bayes' Theory which says:\n",
    "\n",
    "$$P(category | data) \\underbrace{\\propto}_\\text{is proportional to} \\underbrace{P(Data | Category)}_{\\text{How common this combination of predictors is for that Category}^1} * \\underbrace{P(Category)}_\\text{How common that category is in the dataset}$$\n",
    "\n",
    "\n",
    "$^1$\n",
    "For example, what is the probability that someone is diabetic, obsese, and a smoker given that they have heart disease.\n",
    "\n",
    "### 0.3 NB in sklearn\n",
    "In sklearn there are 3 main functions you can use to perform Naive Bayes:\n",
    "\n",
    "* `GaussianNB()`: Assumes that features follow a Normal/Gaussian Distribution.\n",
    "* `BernoulliNB()`: Assumes features are binary (0/1)\n",
    "* `CategoricalNB()`: Assumes features are discrete categories (can have more than 2 categories)\n",
    "\n",
    "This means that if your features are continuous you'd use `GaussianNB()`, if they are only binary, use `BernoulliNB()` and if they are only Categorical, use `CategoricalNB()`. In practice, we'll often use either `GaussianNB()` or `CategoricalNB()` (since `CategoricalNB()` can also handle it when we have binary + categorical).\n",
    "\n",
    "This means that computationally, we cannot have both continuous + categorical predictors in one sklearn NB model. (There are workarounds for this: see [here](https://stackoverflow.com/questions/14254203/mixing-categorial-and-continuous-data-in-naive-bayes-classifier-using-scikit-lea), but for now, we'll be using only one or the other).\n",
    "\n",
    "\n",
    "## 1. Naive Bayes By Hand\n",
    "\n",
    "### 1.1 Calculating Probabilities for Each Category\n",
    "\n",
    "The dataframe `d` below, is a (fake) dataset that we'll use to predict whether someone owns a home or not (the `own` column). For each outcome category (own-`1`, not own-`0`) calculate the probability of having a `1` in each of the predictor categories (having an income > 100k, being over 40, having kids, and having more than one income).\n",
    "\n",
    "\n",
    "Store these probabilities in a dataframe. The dataframe should look like the table below, but with the actual probabilities instead of 1's.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1imX0dbPjiEy56kruM8c86A3wA1EqpA8Q\" width = 250px/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>own</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>incomeOver100k</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ageOver40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kids</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>morethan1Income</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             names  own  not\n",
       "0   incomeOver100k  0.8  0.8\n",
       "1        ageOver40  1.0  0.7\n",
       "2             kids  0.8  0.3\n",
       "3  morethan1Income  0.7  0.9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/HomeOwnership.csv\")\n",
    "d.head()\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "d_own = d.loc[d.own == 1]\n",
    "d_not = d.loc[d.own == 0]\n",
    "\n",
    "colNames = [\"incomeOver100k\", \"ageOver40\", \"kids\", \"morethan1Income\"]\n",
    "\n",
    "d_own_ps = [d_own[x].mean() for x in colNames]\n",
    "d_not_ps = [d_not[x].mean() for x in colNames]\n",
    "\n",
    "df = pd.DataFrame({\"names\": colNames,\n",
    "                  \"own\": d_own_ps,\n",
    "                  \"not\": d_not_ps})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Predicting Category\n",
    "Using the formula we learned in the Naive Bayes lecture, choose which category (own-`1` or not own-`0`) the following two people should be classified as:\n",
    "\n",
    "| incomeOver100k | ageOver40 | kids | morethan1Income |\n",
    "|----------------|-----------|------|-----------------|\n",
    "| 0              | 1         | 1    | 0               |\n",
    "| 1              | 1         | 0    | 1               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1 should be in category 1 for home ownership.\n",
      "Person 2 should be in category 0 for home ownership.\n",
      "0.024 0.002099999999999999 0.05599999999999999 0.17639999999999997\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "prop_own = d.own.mean()\n",
    "prop_not = 1- prop_own\n",
    "\n",
    "person_1 = np.array([0,1,1,0])\n",
    "person_2 = np.array([1,1,0,1])\n",
    "\n",
    "score_own_1 = np.product(person_1*df.own + (1-person_1)*(1-df.own)) * prop_own\n",
    "score_own_2 = np.product(person_2*df.own + (1-person_2)*(1-df.own)) * prop_own\n",
    "\n",
    "score_not_1 = np.product(person_1*df[\"not\"] + (1-person_1)*(1-df[\"not\"])) * prop_not\n",
    "score_not_2 = np.product(person_2*df[\"not\"] + (1-person_2)*(1-df[\"not\"])) * prop_not\n",
    "\n",
    "print(\"Person 1 should be in category\", int(score_own_1 > score_not_1), \"for home ownership.\")\n",
    "print(\"Person 2 should be in category\", int(score_own_2 > score_not_2), \"for home ownership.\")\n",
    "print(score_own_1,score_not_1, score_own_2, score_not_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build a NB in sklearn\n",
    "\n",
    "#### *Question*\n",
    "Now, using d, build a naive bayes model using `d` (no need for model validation here). Then use the `.predict()` function to predict the category for the two people from 1.2. Does the models predicted category match the one you did by hand?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1 should be in category 1 for home ownership.\n",
      "Person 2 should be in category 0 for home ownership.\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# hint: to predict a single data point, use: data_point = np.array(dp).reshape(1,-1), where dp is a list with\n",
    "# the predictor values, and then call .predict(data_point) on your model \n",
    "\n",
    "# Use BernoulliNB or CategoricalNB since we have categorical variables\n",
    "\n",
    "nb = BernoulliNB()\n",
    "\n",
    "nb.fit(d[colNames], d[\"own\"])\n",
    "\n",
    "person_1_predict = nb.predict(person_1.reshape(1,-1))\n",
    "person_2_predict = nb.predict(person_2.reshape(1,-1))\n",
    "\n",
    "print(\"Person 1 should be in category\", person_1_predict[0], \"for home ownership.\")\n",
    "print(\"Person 2 should be in category\", person_2_predict[0], \"for home ownership.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build a CONTINUOUS NB in sklearn\n",
    "\n",
    "While we won't do the math by hand for the continous (Gaussian) version of Naive Bayes, let's practice running it in sklearn.\n",
    "\n",
    "Using the `diabetes` dataset, create and fit a `GaussianNB()` model to predict whether or not someone has diabetes (`1`-diabetes, `0`-no diabetes). Use Train Test Split an evaluate how well your model does on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/diabetes2.csv\")\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7662337662337663"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "predictors = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\",\n",
    "                  \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "# Use GaussianNB because we have all continuous predictors\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes[predictors],diabetes[\"Outcome\"], test_size = 0.2)\n",
    "\n",
    "z = StandardScaler()\n",
    "X_train[predictors] = z.fit_transform(X_train[predictors])\n",
    "X_test[predictors] = z.transform(X_test[predictors])\n",
    "\n",
    "nb2 = GaussianNB()\n",
    "\n",
    "nb2.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, nb2.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Being Naive is...good!\n",
    "\n",
    "We mentioned in lecture why the naive assumption in NB is useful, computationally. But now, it's your turn to experience it first hand! Using the LARGER home ownership dataset `d2`, first calculate the probability $P(1,0,1,1)$ (where `[1,0,1,1]` represents a person's values for the 4 predictors, `incomeOver100k`, `ageOVer40`, `kids`, and `morethan1Income`) the **naive** way for *both* home owners and non-owners. \n",
    "\n",
    "$$ P(A,B,C,D) = P(A)*P(B)*P(C)*P(D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14763936000000003 0.079704\n"
     ]
    }
   ],
   "source": [
    "d2 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/HomeOwnership2.csv\")\n",
    "d2.head()\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# find the probabilities:\n",
    "d2_own = d2.loc[d2.own == 1]\n",
    "d2_not = d2.loc[d2.own == 0]\n",
    "\n",
    "colNames = [\"incomeOver100k\", \"ageOver40\", \"kids\", \"morethan1Income\"]\n",
    "\n",
    "d2_own_ps = [d2_own[x].mean() for x in colNames]\n",
    "d2_not_ps = [d2_not[x].mean() for x in colNames]\n",
    "\n",
    "df2 = pd.DataFrame({\"names\": colNames,\n",
    "                  \"own\": d2_own_ps,\n",
    "                  \"not\": d2_not_ps})\n",
    "\n",
    "person = np.array([1,0,1,1])\n",
    "\n",
    "#p(1,0,1,1) for homeowners\n",
    "score_own = np.product(person*df2.own + (1-person)*(1-df2.own)) \n",
    "\n",
    "#p(1,0,1,1) for non-homeowners\n",
    "score_not = np.product(person*df2[\"not\"] + (1-person)*(1-df2[\"not\"]))\n",
    "\n",
    "\n",
    "print(score_own, score_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate $P(1,0,1,1 | \\text{own})$ (where `[1,0,1,1]` represents a person's values for the 4 predictors, `incomeOver100k`, `ageOVer40`, `kids`, and `morethan1Income`) the **regular** way for *both* home owners and non-owners. \n",
    "\n",
    "Using the *chain rule* of probabilities, the probability of multiple events, $P(A,B,C,D)$ is equal to:\n",
    "\n",
    "$$P(A,B,C,D)= P(A|B,C,D)*P(B|C,D)*P(C|D)*P(D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15000000000000002"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "#p(1,0,1,1) for homeowners\n",
    "pABCD = d2_own.loc[(d2_own.morethan1Income == 1) & (d2_own.kids == 1) & (d2_own.ageOver40 == 0)].incomeOver100k.mean()\n",
    "pBCD = 1 - (d2_own.loc[(d2_own.morethan1Income == 1) & (d2_own.kids == 1) ].ageOver40.mean()) # use 1 - because this one has a 0 value\n",
    "pCD = d2_own.loc[d2_own.morethan1Income == 1].kids.mean()\n",
    "pD = d2_own.morethan1Income.mean()\n",
    "\n",
    "p = pABCD*pBCD*pCD*pD\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08000000000000002"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p(1,0,1,1) for non-homeowners\n",
    "pABCD2 = d2_not.loc[(d2_not.morethan1Income == 1) & (d2_not.kids == 1) & (d2_not.ageOver40 == 0)].incomeOver100k.mean()\n",
    "pBCD2 = 1 - (d2_not.loc[(d2_not.morethan1Income == 1) & (d2_not.kids == 1) ].ageOver40.mean()) # use 1 - because this one has a 0 value\n",
    "pCD2 = d2_not.loc[d2_not.morethan1Income == 1].kids.mean()\n",
    "pD2 = d2_not.morethan1Income.mean()\n",
    "\n",
    "p2 = pABCD2*pBCD2*pCD2*pD2\n",
    "p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much simpler the naive way is?? and this is a TINY dataset with very few features."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
