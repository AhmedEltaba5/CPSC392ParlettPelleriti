{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "from sklearn.model_selection import cross_val_score # cross validation metrics\n",
    "from sklearn.model_selection import cross_val_predict # cross validation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Ensemble\n",
    "A common theme in applied Machine Learning is *The Ensemble Method*. Ensemble methods use multiple machine learning models (these models can be the same type or different algorithms entirely). The idea is that using ensembles improves predictive performance, because even though our models are sometimes incorrect, it's unlikely that a MAJORITY of the models in our ensemble will all be incorrect in the exact same way each time. Therefore in aggregate, we will get a more accurate model.\n",
    "\n",
    "Each model gets a \"vote\" about what category a data point should be in (ensemble methods also work for continuous outcomes, but here we'll focus on categorical ones). Whichever category gets the most \"votes\" is the category we choose for that data point. \n",
    "\n",
    "To combat overfitting and reduce potential *over-reliance* on a small number of features, we can use the two following techniques when creating models for our ensemble:\n",
    "\n",
    "* **Bagging (Bootstrap Aggregating)**: Instead of using all of our training data to train each model in our sample we use **bootstrapping** to choose the samples we will include.\n",
    "    * **Bootstrapping** is when you randomly sample data points *with replacement*, meaning that a data point can be included in your bootstrapped sample *more* than once, OR not at all.\n",
    "* **Random Feature Selection**: Instead of using all the available features/predictors in our dataset for every model, for each model we randomly choose a different subset of features to use when training. This helps our ensemble generalize, because it doesn't become overly reliant on one feature (since that feature might not appear in every model).\n",
    "\n",
    "While ensemble methods take a lot of computational power (you're training MANY models instead of just one), in practice they're often really useful. An incredibly popular ensemble method is the **Random Forest** which is an ensemble method that uses a bunch of decision trees along with Bagging and Random Feature selection to generate the ensemble.\n",
    "\n",
    "## 1.1 Building a Random Forest\n",
    "\n",
    "Let's build a tiny random forest function of our own! Write a function `Forest()` that takes in 6 arguments:\n",
    "\n",
    "* `n_samples` (**integer**): number of bootstrapped samples to use to train each decision tree.\n",
    "* `n_features` (**integer**): number of randomly selected features from your data set to use when training.\n",
    "* `n_trees` (**integer**): how many decision trees to create for the ensemble.\n",
    "* `max_depth` (**integer**): the max_depth for all of your trees.\n",
    "* `X` (**data frame**): the *already* z-scored predictor data to be used.\n",
    "* `y` (**data frame**): the outcome data to be used (`X` and `y` are the same length, and the $i^{th}$ element of `X` corresponds to the $i^{th}$ element of `y`)\n",
    "\n",
    "The function should:\n",
    "\n",
    "1. use a for loop to create `n_trees` models and store them in a list called `forest` (yes! You can store fitted decision trees in a list!)\n",
    "2. For each model you should choose use bootstrapping to sample `n_samples` data points to train each model. Remember that boostrapping means sampling WITH replacement (hint: try using [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) to randomly select (*with replacement*) which row numbers/indices to use.\n",
    "3. For each model, randomly select `n_features` to use to train your model. (hint: try using [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) to randomly select (*withOUT replacement*) which predictor indices to use.\n",
    "4. For all models, make sure you set the `max_depth`.\n",
    "5. For each model, train the model (no need to use model validation, and assume X is already z-scored).\n",
    "6. Return a list (`forest`) of dictonaries that look like this (where `tree` is your trained model and `samples_index` is an array of indices for the features/predictors you selected):\n",
    " ```{\"tree\": tree, \"feats\": samples_index}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jasper</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Luke</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Susan</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lisa</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kayne</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lydia</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jasper</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lisa</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>John</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Anthony</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Jane</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Leia</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Peter</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Greg</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  age\n",
       "14   Jasper   26\n",
       "19     Luke   19\n",
       "7     Susan   24\n",
       "28     Lisa   20\n",
       "10    Kayne   25\n",
       "11    Lydia   17\n",
       "14   Jasper   26\n",
       "28     Lisa   20\n",
       "17     John   17\n",
       "23  Anthony   17\n",
       "13     Jane   17\n",
       "20     Leia   23\n",
       "0      Alex   20\n",
       "12    Peter   22\n",
       "5      Greg   26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll Just Leave this hint here...\n",
    "\n",
    "## simple bootstrapping example of names dataframe \n",
    "np.random.seed(1234)\n",
    "\n",
    "names = [\"Alex\", \"Charlie\", \"Addison\", \"James\", \"Blake\", \"Greg\", \"Daniel\", \"Susan\", \"Erik\", \"Georgia\", \"Kayne\",\n",
    "         \"Lydia\", \"Peter\", \"Jane\", \"Jasper\", \"Link\", \"Rhett\", \"John\", \"Miranda\", \"Luke\", \"Leia\", \"Janet\", \"Jung\",\n",
    "         \"Anthony\", \"Mark\", \"Torrence\", \"Bonnie\", \"Rudy\", \"Lisa\", \"Bart\", \"Tina\", \"Marie\"]\n",
    "\n",
    "names_df = pd.DataFrame({\"name\": names, \"age\": np.random.randint(17,27, len(names))})\n",
    "names_df\n",
    "\n",
    "names_index = np.random.choice(range(0,len(names)), 15, replace = True)\n",
    "names_boot = names_df.iloc[names_index]\n",
    "\n",
    "# notice how Lisa shows up more than once?\n",
    "\n",
    "names_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "def Forest(X, y, n_samples = 1000, n_features = 5, n_trees = 100, max_depth = 5):\n",
    "    forest = []\n",
    "    \n",
    "    # create models\n",
    "    for i in range(0,n_trees):\n",
    "        \n",
    "        # 1. randomly bootstrap datapoints by selecting from X's row indices WITH replacement\n",
    "\n",
    "        # 2. randomly choose features by selecting from X's column indices WITHOUT replacement\n",
    "     \n",
    "        # 3. subset X and y to only include the rows and features that were randomly selected above\n",
    "        \n",
    "        # add trained tree and feature/column indices (from 2 above) to forest dict\n",
    "        #forest.append({\"tree\": tree, \"feats\": features_index})\n",
    "    return(forest)\n",
    "### /YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forest(X, y, n_samples = 1000, n_features = 5, n_trees = 100, max_depth = 5):\n",
    "    forest = []\n",
    "    \n",
    "    # create models\n",
    "    for i in range(0,n_trees):\n",
    "        \n",
    "        # randomly bootstrap datapoints\n",
    "        samples_index = np.random.choice(range(0,X.shape[0]), n_samples, replace = True)\n",
    "        \n",
    "        # randomly choose features\n",
    "        if n_features >= X.shape[1]: #if they ask for more features than you have...\n",
    "            features_index = range(0,X.shape[1])\n",
    "        else:\n",
    "            features_index = np.random.choice(range(0,X.shape[1]), n_features, replace = False)\n",
    "        \n",
    "        # select only the rows and features that were randomly selected above\n",
    "        X_bagged = X.iloc[samples_index, features_index]\n",
    "        y_bagged = y.iloc[samples_index]\n",
    "        \n",
    "        tree = DecisionTreeClassifier(max_depth = max_depth)\n",
    "        tree.fit(X_bagged,y_bagged)\n",
    "        \n",
    "        # add tree to forest\n",
    "        forest.append({\"tree\": tree, \"feats\": features_index})\n",
    "    return(forest)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Use `Forest()`\n",
    "Using `X_cols_df` and `y_df` (data generated at the top of the notebook) as your training set, call `Forest()` to build an ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X240</th>\n",
       "      <th>X241</th>\n",
       "      <th>X242</th>\n",
       "      <th>X243</th>\n",
       "      <th>X244</th>\n",
       "      <th>X245</th>\n",
       "      <th>X246</th>\n",
       "      <th>X247</th>\n",
       "      <th>X248</th>\n",
       "      <th>X249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.546514</td>\n",
       "      <td>-1.563641</td>\n",
       "      <td>1.115903</td>\n",
       "      <td>0.952985</td>\n",
       "      <td>0.192271</td>\n",
       "      <td>0.973264</td>\n",
       "      <td>0.553697</td>\n",
       "      <td>1.170051</td>\n",
       "      <td>-0.454866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934221</td>\n",
       "      <td>-1.226211</td>\n",
       "      <td>1.376160</td>\n",
       "      <td>-1.456604</td>\n",
       "      <td>-1.236548</td>\n",
       "      <td>0.193204</td>\n",
       "      <td>-1.266364</td>\n",
       "      <td>0.338426</td>\n",
       "      <td>0.867695</td>\n",
       "      <td>0.688788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.120423</td>\n",
       "      <td>-1.405240</td>\n",
       "      <td>0.970508</td>\n",
       "      <td>1.660086</td>\n",
       "      <td>-0.606927</td>\n",
       "      <td>1.416359</td>\n",
       "      <td>-0.514392</td>\n",
       "      <td>1.881836</td>\n",
       "      <td>-0.625070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965089</td>\n",
       "      <td>-1.143669</td>\n",
       "      <td>1.615641</td>\n",
       "      <td>-0.872585</td>\n",
       "      <td>-1.053896</td>\n",
       "      <td>0.009002</td>\n",
       "      <td>-1.679799</td>\n",
       "      <td>1.281591</td>\n",
       "      <td>0.761152</td>\n",
       "      <td>0.564718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.594233</td>\n",
       "      <td>-1.655526</td>\n",
       "      <td>1.578430</td>\n",
       "      <td>1.440725</td>\n",
       "      <td>0.097003</td>\n",
       "      <td>-0.297129</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.785563</td>\n",
       "      <td>-0.831346</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202674</td>\n",
       "      <td>-1.198594</td>\n",
       "      <td>1.027653</td>\n",
       "      <td>-1.789076</td>\n",
       "      <td>-0.627827</td>\n",
       "      <td>-0.033458</td>\n",
       "      <td>-1.243939</td>\n",
       "      <td>1.105503</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>1.169202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.387366</td>\n",
       "      <td>-1.363140</td>\n",
       "      <td>1.026211</td>\n",
       "      <td>2.032231</td>\n",
       "      <td>-0.059808</td>\n",
       "      <td>0.456470</td>\n",
       "      <td>0.751682</td>\n",
       "      <td>2.235540</td>\n",
       "      <td>-0.490625</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064223</td>\n",
       "      <td>-1.743916</td>\n",
       "      <td>1.163846</td>\n",
       "      <td>-1.939953</td>\n",
       "      <td>-0.263838</td>\n",
       "      <td>0.387277</td>\n",
       "      <td>-1.787271</td>\n",
       "      <td>1.297693</td>\n",
       "      <td>0.571830</td>\n",
       "      <td>1.263347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.027355</td>\n",
       "      <td>-1.657837</td>\n",
       "      <td>2.383260</td>\n",
       "      <td>1.375557</td>\n",
       "      <td>-0.358580</td>\n",
       "      <td>1.469940</td>\n",
       "      <td>-0.190193</td>\n",
       "      <td>1.236663</td>\n",
       "      <td>-0.730892</td>\n",
       "      <td>...</td>\n",
       "      <td>1.352485</td>\n",
       "      <td>-1.440584</td>\n",
       "      <td>1.437159</td>\n",
       "      <td>-1.355588</td>\n",
       "      <td>-0.708502</td>\n",
       "      <td>0.050739</td>\n",
       "      <td>-1.910396</td>\n",
       "      <td>1.089436</td>\n",
       "      <td>0.498450</td>\n",
       "      <td>0.767962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        X0        X1        X2        X3        X4        X5  \\\n",
       "0           0 -0.546514 -1.563641  1.115903  0.952985  0.192271  0.973264   \n",
       "1           1  0.120423 -1.405240  0.970508  1.660086 -0.606927  1.416359   \n",
       "2           2  0.594233 -1.655526  1.578430  1.440725  0.097003 -0.297129   \n",
       "3           3  0.387366 -1.363140  1.026211  2.032231 -0.059808  0.456470   \n",
       "4           4 -0.027355 -1.657837  2.383260  1.375557 -0.358580  1.469940   \n",
       "\n",
       "         X6        X7        X8  ...      X240      X241      X242      X243  \\\n",
       "0  0.553697  1.170051 -0.454866  ...  0.934221 -1.226211  1.376160 -1.456604   \n",
       "1 -0.514392  1.881836 -0.625070  ...  0.965089 -1.143669  1.615641 -0.872585   \n",
       "2  0.154600  0.785563 -0.831346  ...  1.202674 -1.198594  1.027653 -1.789076   \n",
       "3  0.751682  2.235540 -0.490625  ...  1.064223 -1.743916  1.163846 -1.939953   \n",
       "4 -0.190193  1.236663 -0.730892  ...  1.352485 -1.440584  1.437159 -1.355588   \n",
       "\n",
       "       X244      X245      X246      X247      X248      X249  \n",
       "0 -1.236548  0.193204 -1.266364  0.338426  0.867695  0.688788  \n",
       "1 -1.053896  0.009002 -1.679799  1.281591  0.761152  0.564718  \n",
       "2 -0.627827 -0.033458 -1.243939  1.105503  0.848282  1.169202  \n",
       "3 -0.263838  0.387277 -1.787271  1.297693  0.571830  1.263347  \n",
       "4 -0.708502  0.050739 -1.910396  1.089436  0.498450  0.767962  \n",
       "\n",
       "[5 rows x 251 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cols_df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/X_cols_df.csv\")\n",
    "y_df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/y_df.csv\")\n",
    "X_cols_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "my_forest = ### call Forest and create an ensemble.\n",
    "\n",
    "### /YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Comparing Ensemble to an Individual Model\n",
    "\n",
    "- Use the `ForestPredictor()` function below (which takes in the ensemble created by `Forest()` and data) to generate predictions for `X_cols_df2`, our *test* set.\n",
    "- Use the `ForestPredictor()` function below (which takes in the ensemble created by `Forest()` and data) to generate predictions for `X_cols_df`, our *train* set.\n",
    "- calculate the accuracy of your ensemble.\n",
    "- calculate the accuracy for ONE of your ensemble models by using `oneModel = my_forest[0]` to grab the first model of your ensemble. \n",
    "\n",
    "### 1.3.1\n",
    "In this example, does an ensemble method do *better* (in terms of train accuracy) than an individual decision tree? Explain how you figured this out.\n",
    "\n",
    "### 1.3.2\n",
    "In this example, does an ensemble method do *better* (in terms of overfitting) than an individual decision tree? Use `X_cols_df2` and `y_df2` as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForestPredictor(forest, X):\n",
    "# takes in a list of dictionaries like this but longer:\n",
    "# [\n",
    "#     {\"trees\":DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "#      max_depth=5, max_features=None, max_leaf_nodes=None,\n",
    "#      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#      min_samples_leaf=1, min_samples_split=2,\n",
    "#      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "#      random_state=None, splitter='best'),\n",
    "#     \"feats\": array([ 63, 101,  39, 133, 137])},\n",
    "\n",
    "#  {\"trees\":DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
    "#      max_depth=5, max_features=None, max_leaf_nodes=None,\n",
    "#      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#      min_samples_leaf=1, min_samples_split=2,\n",
    "#      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "#      random_state=None, splitter='best'),\n",
    "#     \"feats\": array([ 63, 101,  39, 133, 137])}\n",
    "# ]\n",
    "    import operator\n",
    "    from collections import Counter\n",
    "\n",
    "    X = X_cols_df\n",
    "    ps = []\n",
    "\n",
    "    # get predictions from each model\n",
    "    for model in forest:\n",
    "        tree = model[\"tree\"]\n",
    "        X_sub = X.iloc[:, model[\"feats\"]]\n",
    "\n",
    "        p = tree.predict(X_sub)\n",
    "        ps.append(p)\n",
    "\n",
    "    ps = pd.DataFrame(ps)\n",
    "    \n",
    "    # get ensemble prediction for each data point\n",
    "    predictions = []\n",
    "    \n",
    "    for column_ind in range(0, ps.shape[1]):\n",
    "        ensemble_predict = ps.iloc[:,column_ind]\n",
    "        predictions.append(ensemble_predict.mode()[0])\n",
    "\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n",
    "# ForestPredict() will take your ensemble and use it to find the predicted values for X_cols_df2\n",
    "ensemble_predictions =  ### Call ForestPredictor using my_forest and X_cols_df2\n",
    "\n",
    "### /YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# calculate the accuracy for the ensemble\n",
    "\n",
    "\n",
    "# calculate the accuracy for the first model\n",
    "oneModel = my_forest[0]\n",
    "\n",
    "\n",
    "### /YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Comparing Ensemble to an Individual ModelS\n",
    "\n",
    "- put the accuracy from your ENSEMBLE model in the code below\n",
    "- run the cell to see a histogram of the individual tree accuracies, and the (dashed line) ensemble accuracy.\n",
    "\n",
    "### 1.4.1\n",
    "Write down your thoughts about this graph. What patterns do you see between individual tree accuracies and ensemble accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-658082e9cbc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### /YOUR CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mallAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_df2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_forest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tree\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cols_df2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_forest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feats\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_forest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mallAcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"no\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_forest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_forest' is not defined"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "ensemble_acc = 0.775### put your ensemble accuracy here!\n",
    "\n",
    "### /YOUR CODE HERE ###\n",
    "\n",
    "allAcc = [accuracy_score(y_df2,my_forest[mod][\"tree\"].predict(X_cols_df2.iloc[:,my_forest[mod][\"feats\"]])) for mod in range(0,len(my_forest))]\n",
    "\n",
    "df = pd.DataFrame({\"acc\": allAcc, \"no\": range(0,len(my_forest))})\n",
    "(ggplot(df, aes(x = \"acc\")) +\n",
    " geom_histogram(color = \"black\", fill = \"lightblue\", binwidth = 0.025) +\n",
    " xlim([0,1]) + theme_minimal() + geom_vline(xintercept = ensemble_acc, linetype = \"dashed\", size = 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2\n",
    "How does the difference between individual tree accuracies and ensemble accuracies change when you change the number of predictors used in each tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a9b76e94e689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmy_forest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cols_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mensemble_acc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_df2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForestPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cols_df2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0d66ae05a37f>\u001b[0m in \u001b[0;36mForest\u001b[0;34m(X, y, n_samples, n_features, n_trees, max_depth)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_bagged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# add tree to forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \"\"\"\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    168\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be class 'SparseSeries' or 'SparseArray'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_multilabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'multilabel-indicator'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    147\u001b[0m                  _is_integral_float(np.unique(y.data))))\n\u001b[1;32m    148\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         return len(labels) < 3 and (y.dtype.kind in 'biu' or  # bool, int, uint\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "n_feat = 100\n",
    "### /YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "my_forest2 = Forest(X_cols_df, y_df, n_features = n_feat)\n",
    "\n",
    "ensemble_acc2 = accuracy_score(y_df2, ForestPredictor(forest, X_cols_df2))\n",
    "\n",
    "### /YOUR CODE HERE ###\n",
    "\n",
    "allAcc2 = [accuracy_score(y_df2,my_forest2[mod][\"tree\"].predict(X_cols_df2.iloc[:,my_forest2[mod][\"feats\"]])) for mod in range(0,len(my_forest2))]\n",
    "\n",
    "df = pd.DataFrame({\"acc\": allAcc2, \"no\": range(0,len(my_forest2))})\n",
    "(ggplot(df, aes(x = \"acc\")) +\n",
    " geom_histogram(color = \"black\", fill = \"lightblue\", binwidth = 0.025) +\n",
    " xlim([0,1]) + theme_minimal() + geom_vline(xintercept = ensemble_acc, linetype = \"dashed\", size = 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be graded on 1) the correctness of your code 2) the answers to the questions."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
